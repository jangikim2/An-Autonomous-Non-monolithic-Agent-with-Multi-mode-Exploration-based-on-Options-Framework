Our research takes advantage of HIRO [1] for the implementation of an options framework. The reference code of HIRO is open source code, https://github.com/ziangqin-stu/rl_hiro. In addition, PPO open source \textcolor{black}{code} is also adopted, https://github.com/nikhilbarhate99/PPO-PyTorch. Finally, those two open \textcolor{black}{programs} are combined to implement our model. In the Algorithm 1\textsuperscript{\ref{alg:the_alg}}, we represent only the main part of our algorithm which is implemented on the reference code. 

In this implementation, we try to take advantage of the original parameter setting of reference codes, HIRO and PPO which are mentioned above. The size of neural network of actor and critic for the exploration policy, $\pi^{PPO}_{T}$, and on-policy, $\pi^{PPO}_{M}$, are the same as the that of $\pi^{TD3}_{M}$ and $\pi^{TD3}_{L}$. The size of neural network of $\pi^{TD3}_{M}$ and $\pi^{TD3}_{L}$ is the same as that of neural network which HIRO utilizes. All training parameters of $\pi^{TD3}_{M}$ and $\pi^{TD3}_{L}$ are used with the parameters defined on 'train\_param\_hiro.csv' file. The training parameters of the exploration policy, $\pi^{PPO}_{T}$, and on-policy, $\pi^{PPO}_{M}$ are adopted with the same ones from PPO reference code above except for K\_epochs which is changed from 80 to 10. The value of horizon of the exploration policy, $\pi^{PPO}_{T}$ is 50 steps. Its training step is 400 steps. The value of horizon of $\pi^{PPO}_{M}$ is the same as that of $\pi^{TD3}_{M}$. The training step of $\pi^{PPO}_{M}$ is 30 steps. The K\_epochs of $\pi^{PPO}_{M}$ is also 10. The size of action and state are the same as those of Ant domain.

Ant Push and Ant Fall in our implementation are tested until 3.15M and 6M steps respectively. During the first 100K steps, \textit{Middle} and \textit{Low} levels are trained according to the intention of the original code. Then, during the next 250K steps, only random policy runs and all levels including \textit{Top} level are trained. After only random policy runs, all modes which are random policy, on-policy and off-policy work. All logs start after the first 100K steps.

The target rates, $\rho$, regarding the reference models of \cite{63} are as follows. In Ant Push, the $\rho$ of Ref:PPO and Ref:Uniform random are 0.01 and 0.0001 respectively. In Ant Fall, the $\rho$ of Ref:PPO and Ref:Uniform random are the same, at 0.001.

The following parameters are the parameters of our model. $S\_O_{g^{\text{expl-mode}}}$ of both Ant Push and Ant Fall in the starting mode is 0.9. Then, After starting mode, $S\_O_{g^{\text{expl-mode}}}$ is changed as follows. In Ant Push, $S\_O_{g^{\text{expl-mode}}}$ of all three sub-policies,$\;\pi^{TD3}, \pi^{PPO}\;and\\\pi^{RND}$, of \textit{Middle} level is the same as 0.6. In Ant Fall, $S\_O_{g^{\text{expl-mode}}}$ of $\pi^{TD3}\;\text{and}\;\pi^{PPO}$ is 0.6. $S\_O_{g^{\text{expl-mode}}}$ of $\pi^{RND}$ is still 0.9. Meanwhile, In Ant Push, $\alpha_{g^{\text{expl-mode}}}$ of $\pi^{TD3}, \pi^{PPO}\;and\;\pi^{RND}$ are 0, 0.4 and 0.7 respectively. In Ant Fall, $\alpha_{g^{\text{expl-mode}}}$ of $\pi^{TD3}, \pi^{PPO}\;and\;\pi^{RND}$ are -0.2, -0.2 and 0.7 respectively.

Furthermore, the sign of success rate, $S\_E$, is positive for $\pi^{PPO}$ and $\pi^{RND}$ and negative for $\pi^{TD3}$ in both tasks. The purpose of $S\_E$ is to increase or reduce the loss of $\pi^{PPO}_{T}$ regarding the original loss value.

The functions used in Algorithm 1 \textsuperscript{\ref{alg:the_alg}} are as follows. $Clamp\_Max(...)$ is the function of clamping the action of \textit{Low} level into the max\_value range. $Train_{T}(...,S\_E,g^{\text{expl-mode}},...)$ is the train function of \textit{Top} level. $\textit{Train\_$\pi^{TD3}_{M}(...)$}$ is the train function of $\pi^{TD3}_{M}$ of \textit{Middle} level. $\textit{Train\_$\pi^{PPO}_{M}(...)$}$ is the train function of $\pi^{PPO}_{M}$ of \textit{Middle} level. The evaluation function of $\pi^{TD3}$ of \textit{Middle} level is $Evaluate\_\pi^{TD3}_{M}(...)$. $Judge\_success(...)$ is the judge function regarding the success of $\pi^{TD3}$ of \textit{Middle} level. The higher-level context, $target\_pos$, is used in common on the policies of \textit{Top} and \textit{Middle}.

The model used in the ablation study is the same as the parameters of the model used in Ant Push. The condition of each case is applied to implement it.

[1] O. Nachum, S. Gu, H. Lee, and S. Levine, “Data-efficient hierarchical reinforcement learning,” arXiv preprint arXiv:1805.08296, 2018.